{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_out = '''\n",
    "{\n",
    " \"sub_process_list\": [\n",
    "{\n",
    " \"start\": \"Capacity monitoring begins\",\n",
    " \"condition\": \"\",\n",
    " \"sequence_of_tasks\": [\"Monitor capacity utilization with BackEnd OPC\", \"Check 13-week and 26-week horizon\"],\n",
    " \"end\": \"Loading drops below threshold?\"\n",
    "},\n",
    "{\n",
    " \"start\": \"Loading drops below threshold?\",\n",
    " \"condition\": \"Yes\",\n",
    " \"sequence_of_tasks\": [\"Trigger assessment to determine cause of underloading\"],\n",
    " \"end\": \"Assess cause of underloading\"\n",
    "},\n",
    "{\n",
    " \"start\": \"Loading drops below threshold?\",\n",
    " \"condition\": \"No\",\n",
    " \"sequence_of_tasks\": [\"Continue monitoring capacity utilization\"],\n",
    " \"end\": \"Capacity monitoring continues\"\n",
    "},\n",
    "{\n",
    " \"start\": \"Assess cause of underloading\",\n",
    " \"condition\": \"Sub-bottleneck identified\",\n",
    " \"sequence_of_tasks\": [\"Investigate sub-bottlenecks or machine constraints\", \"Coordinate with BackEnd OPC to increase limiting capacity\"],\n",
    " \"end\": \"Coordinate with BackEnd OPC\"\n",
    "},\n",
    "{\n",
    " \"start\": \"Assess cause of underloading\",\n",
    " \"condition\": \"FrontEnd chip shortages\",\n",
    " \"sequence_of_tasks\": [\"Compare constrained plan against unconstrained plan\", \"Request SCP to increase FrontEnd chip supply\", \"Send supporting emails to pull chips into DieBank\"],\n",
    " \"end\": \"Increase FrontEnd chip supply\"\n",
    "},\n",
    "{\n",
    " \"start\": \"Assess cause of underloading\",\n",
    " \"condition\": \"Soft demand\",\n",
    " \"sequence_of_tasks\": [\n",
    "\"Compare unconstrained plan to hardware-constrained demand\",\n",
    "\"Review capacity line for non-production days and public holidays\",\n",
    "\"Adjust bottleneck capacity for affected weeks\",\n",
    "\"Pull manual data for constraint and safety stock plans\",\n",
    "\"Calculate required capacity considering safety stocks and previous requests\",\n",
    "\"Adjust required capacity for non-production days and public holidays\",\n",
    "\"Ensure capacity utilization between 91% and 95% on average\",\n",
    "\"Assess capacity line against ATV PP playbook\",\n",
    "\"Initiate email or meeting with SCP to discuss capacity adjustments\",\n",
    "\"Obtain SCM Head approval for adjustments or justifications for maintaining current capacity\"\n",
    " ],\n",
    " \"end\": \"Adjust capacity line\"\n",
    "},\n",
    "{\n",
    " \"start\": \"Coordinate with BackEnd OPC\",\n",
    " \"condition\": \"\",\n",
    " \"sequence_of_tasks\": [\"Update capacity\", \"Trigger DM refresh to reassess utilization and cut rates\"],\n",
    " \"end\": \"DM refresh triggered\"\n",
    "},\n",
    "{\n",
    " \"start\": \"Increase FrontEnd chip supply\",\n",
    " \"condition\": \"\",\n",
    " \"sequence_of_tasks\": [\"Receive updates on FrontEnd chip supply\", \"Reassess utilization post-refresh\"],\n",
    " \"end\": \"DM refresh triggered\"\n",
    "},\n",
    "{\n",
    " \"start\": \"Adjust capacity line\",\n",
    " \"condition\": \"\",\n",
    " \"sequence_of_tasks\": [\"Finalize capacity adjustments\", \"Trigger DM refresh to reassess utilization and cut rates\"],\n",
    " \"end\": \"DM refresh triggered\"\n",
    "},\n",
    "{\n",
    " \"start\": \"DM refresh triggered\",\n",
    " \"condition\": \"\",\n",
    " \"sequence_of_tasks\": [\"Validate that capacity line reflects all adjustments correctly\"],\n",
    " \"end\": \"Validate capacity updates\"\n",
    "},\n",
    "{\n",
    " \"start\": \"Validate capacity updates\",\n",
    " \"condition\": \"Updates align with inputs\",\n",
    " \"sequence_of_tasks\": [\"Ensure alignment with stakeholder inputs\", \"Continue monitoring capacity utilization\"],\n",
    " \"end\": \"Capacity monitoring continues\"\n",
    "},\n",
    "{\n",
    " \"start\": \"Validate capacity updates\",\n",
    " \"condition\": \"Updates do not align\",\n",
    " \"sequence_of_tasks\": [\"Reassess capacity plan with stakeholders\", \"Collaborate to develop improved capacity plan\"],\n",
    " \"end\": \"Reassess capacity plan\"\n",
    "},\n",
    "{\n",
    " \"start\": \"Reassess capacity plan\",\n",
    " \"condition\": \"\",\n",
    " \"sequence_of_tasks\": [\"Finalize reassessed capacity plan\", \"Continue monitoring capacity utilization\"],\n",
    " \"end\": \"Capacity monitoring continues\"\n",
    "}\n",
    " ]\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parsed = json.loads(json_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub_process_list': [{'start': 'Capacity monitoring begins',\n",
       "   'condition': '',\n",
       "   'sequence_of_tasks': ['Monitor capacity utilization with BackEnd OPC',\n",
       "    'Check 13-week and 26-week horizon'],\n",
       "   'end': 'Loading drops below threshold?'},\n",
       "  {'start': 'Loading drops below threshold?',\n",
       "   'condition': 'Yes',\n",
       "   'sequence_of_tasks': ['Trigger assessment to determine cause of underloading'],\n",
       "   'end': 'Assess cause of underloading'},\n",
       "  {'start': 'Loading drops below threshold?',\n",
       "   'condition': 'No',\n",
       "   'sequence_of_tasks': ['Continue monitoring capacity utilization'],\n",
       "   'end': 'Capacity monitoring continues'},\n",
       "  {'start': 'Assess cause of underloading',\n",
       "   'condition': 'Sub-bottleneck identified',\n",
       "   'sequence_of_tasks': ['Investigate sub-bottlenecks or machine constraints',\n",
       "    'Coordinate with BackEnd OPC to increase limiting capacity'],\n",
       "   'end': 'Coordinate with BackEnd OPC'},\n",
       "  {'start': 'Assess cause of underloading',\n",
       "   'condition': 'FrontEnd chip shortages',\n",
       "   'sequence_of_tasks': ['Compare constrained plan against unconstrained plan',\n",
       "    'Request SCP to increase FrontEnd chip supply',\n",
       "    'Send supporting emails to pull chips into DieBank'],\n",
       "   'end': 'Increase FrontEnd chip supply'},\n",
       "  {'start': 'Assess cause of underloading',\n",
       "   'condition': 'Soft demand',\n",
       "   'sequence_of_tasks': ['Compare unconstrained plan to hardware-constrained demand',\n",
       "    'Review capacity line for non-production days and public holidays',\n",
       "    'Adjust bottleneck capacity for affected weeks',\n",
       "    'Pull manual data for constraint and safety stock plans',\n",
       "    'Calculate required capacity considering safety stocks and previous requests',\n",
       "    'Adjust required capacity for non-production days and public holidays',\n",
       "    'Ensure capacity utilization between 91% and 95% on average',\n",
       "    'Assess capacity line against ATV PP playbook',\n",
       "    'Initiate email or meeting with SCP to discuss capacity adjustments',\n",
       "    'Obtain SCM Head approval for adjustments or justifications for maintaining current capacity'],\n",
       "   'end': 'Adjust capacity line'},\n",
       "  {'start': 'Coordinate with BackEnd OPC',\n",
       "   'condition': '',\n",
       "   'sequence_of_tasks': ['Update capacity',\n",
       "    'Trigger DM refresh to reassess utilization and cut rates'],\n",
       "   'end': 'DM refresh triggered'},\n",
       "  {'start': 'Increase FrontEnd chip supply',\n",
       "   'condition': '',\n",
       "   'sequence_of_tasks': ['Receive updates on FrontEnd chip supply',\n",
       "    'Reassess utilization post-refresh'],\n",
       "   'end': 'DM refresh triggered'},\n",
       "  {'start': 'Adjust capacity line',\n",
       "   'condition': '',\n",
       "   'sequence_of_tasks': ['Finalize capacity adjustments',\n",
       "    'Trigger DM refresh to reassess utilization and cut rates'],\n",
       "   'end': 'DM refresh triggered'},\n",
       "  {'start': 'DM refresh triggered',\n",
       "   'condition': '',\n",
       "   'sequence_of_tasks': ['Validate that capacity line reflects all adjustments correctly'],\n",
       "   'end': 'Validate capacity updates'},\n",
       "  {'start': 'Validate capacity updates',\n",
       "   'condition': 'Updates align with inputs',\n",
       "   'sequence_of_tasks': ['Ensure alignment with stakeholder inputs',\n",
       "    'Continue monitoring capacity utilization'],\n",
       "   'end': 'Capacity monitoring continues'},\n",
       "  {'start': 'Validate capacity updates',\n",
       "   'condition': 'Updates do not align',\n",
       "   'sequence_of_tasks': ['Reassess capacity plan with stakeholders',\n",
       "    'Collaborate to develop improved capacity plan'],\n",
       "   'end': 'Reassess capacity plan'},\n",
       "  {'start': 'Reassess capacity plan',\n",
       "   'condition': '',\n",
       "   'sequence_of_tasks': ['Finalize reassessed capacity plan',\n",
       "    'Continue monitoring capacity utilization'],\n",
       "   'end': 'Capacity monitoring continues'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes as input a list of subprocesses: start:condition:sequence:end and trasnforms them into a connected list of elements\n",
    "def parse_json_response_into_elements(parsed_json):\n",
    "  id_counter = 1\n",
    "  shared_id = {}\n",
    "  unique_id = {}\n",
    "  elements = []\n",
    "\n",
    "  for subprocess in parsed_json[\"sub_process_list\"]:\n",
    "    start_element = subprocess[\"start\"]\n",
    "    if start_element not in shared_id.values():\n",
    "      shared_id[f\"{id_counter}\"] = start_element\n",
    "      id_counter += 1\n",
    "\n",
    "    end_element = subprocess[\"end\"]\n",
    "    if end_element not in shared_id.values():\n",
    "      shared_id[f\"{id_counter}\"] = end_element\n",
    "      id_counter += 1\n",
    "\n",
    "    #everything in between start and end is treated as unique item\n",
    "    start_key = next((k for k, v in shared_id.items() if v == start_element), None)\n",
    "    end_key = next((k for k, v in shared_id.items() if v == end_element), None)\n",
    "    start_end_key = f\"{start_key}_{end_key}\"\n",
    "    unique_id[start_end_key] = {}\n",
    "    \n",
    "    condition = subprocess[\"condition\"]\n",
    "    tasks = subprocess[\"sequence_of_tasks\"]\n",
    "    \n",
    "    for task in tasks:\n",
    "      unique_id[start_end_key][id_counter] = task\n",
    "      id_counter += 1\n",
    "\n",
    "    all_tasks = [start_element] + tasks + [end_element]\n",
    "\n",
    "    for i in range(1, len(all_tasks)):\n",
    "      print(\"length of elements\", len(all_tasks))\n",
    "      print(\"current at i\", i)\n",
    "      path_start = False\n",
    "      path_end = False\n",
    "      if i-1 == 0:\n",
    "        path_start = True\n",
    "      if i == len(all_tasks) -1:\n",
    "        path_end = True\n",
    "      \n",
    "      print(path_start, path_end)\n",
    "\n",
    "      from_elem = all_tasks[i-1]\n",
    "      to_elem = all_tasks[i]\n",
    "      outgoing_labels = []\n",
    "      if path_start:\n",
    "        # we fetch from shared keys\n",
    "        start_key = next((k for k, v in shared_id.items() if v == from_elem), None)\n",
    "        outgoing_labels = [condition]\n",
    "      else:\n",
    "        #we fetch from unique keys\n",
    "        start_key = next((k for k, v in unique_id[start_end_key].items() if v == from_elem), None)\n",
    "\n",
    "      if path_end:\n",
    "        #we fetch from shared keys\n",
    "        end_key = next((k for k, v in shared_id.items() if v == to_elem), None)\n",
    "\n",
    "      else:\n",
    "        #we fetch from unique keys\n",
    "        end_key = next((k for k, v in unique_id[start_end_key].items() if v == to_elem), None)\n",
    "\n",
    "      element = {\"id\": start_key, \"type\":\"\", \"label\":from_elem, \"outgoing\":[end_key], \"outgoing_labels\":outgoing_labels}\n",
    "      elements.append(element)\n",
    "\n",
    "    #add end elements without simultanous being start element as new end element\n",
    "    for i in range(1, len(all_tasks)):\n",
    "      if i == len(all_tasks) -1:\n",
    "        path_end = True\n",
    "      \n",
    "      if path_end:\n",
    "        path_end_key = next((k for k, v in shared_id.items() if v == all_tasks[i]), None)\n",
    "        if all_tasks[i] not in [e[\"id\"] for e in elements]:\n",
    "          element = {\"id\": path_end_key, \"type\":\"\", \"label\":all_tasks[i], \"outgoing\":[], \"outgoing_labels\":[]}\n",
    "          elements.append(element)\n",
    "  return elements\n",
    "\n",
    "def densen_connected_elements(elements):\n",
    "    combined = {}\n",
    "\n",
    "    for item in elements:\n",
    "        key = item['id']\n",
    "        if key in combined:\n",
    "            # Combine the 'outgoing' and 'outgoing_labels' lists if id already exists\n",
    "            combined[key]['outgoing'].extend(item['outgoing'])\n",
    "            combined[key]['outgoing_labels'].extend(item['outgoing_labels'])\n",
    "        else:\n",
    "            combined[key] = item.copy()\n",
    "    result_list = list(combined.values())\n",
    "    return result_list\n",
    "\n",
    "def identify_type(dense_elements):\n",
    "    for e in dense_elements:\n",
    "        outgoing_elements = len(e[\"outgoing\"])\n",
    "        if e[\"id\"] == \"1\":\n",
    "            e[\"type\"] = \"startEvent\"\n",
    "        elif outgoing_elements == 0:\n",
    "            e[\"type\"] = \"endEvent\"\n",
    "        elif outgoing_elements == 1:\n",
    "            e[\"type\"] = \"userTask\"\n",
    "        else:\n",
    "            e[\"type\"] = \"exclusiveGateway\"\n",
    "    return dense_elements\n",
    "\n",
    "elements = parse_json_response_into_elements(json_parsed)\n",
    "dense_elements = densen_connected_elements(elements)\n",
    "dense_elements = identify_type(dense_elements)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
